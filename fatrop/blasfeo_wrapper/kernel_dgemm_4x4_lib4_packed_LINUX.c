__asm__(" .text\n\t"
" .p2align 4,,15\n\t"
" .type inner_kernel_dgemm_nt_4x4_lib4, @function; inner_kernel_dgemm_nt_4x4_lib4:\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" movapd 0(%r12), %xmm10\n\t"
"\n\t"
" xorpd %xmm11, %xmm11\n\t"
" movapd %xmm11, %xmm12\n\t"
" movapd %xmm11, %xmm13\n\t"
" movapd %xmm11, %xmm14\n\t"
" movapd %xmm11, %xmm15\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
"\n\t"
" .p2align 3\n\t"
"1:\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 16(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 32(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 32(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 48(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 64(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 64(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 80(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 96(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 96(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 112(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addq $ 128, %r12\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 128, %r11\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 0(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" cmpl $ 4, %r10d\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 0(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
" jg 1b\n\t"
"\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" cmpl $ 3, %r10d\n\t"
" jle 4f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 16(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 32(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 32(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 48(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 64(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 64(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 80(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" movapd 96(%r12), %xmm10\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 96(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 112(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addq $ 128, %r12\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 128, %r11\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
"\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
"\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" addpd %xmm11, %xmm7\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
"\n\t"
"\n\t"
" jmp 2f\n\t"
"\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" movapd 0(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm3\n\t"
" movapd 16(%r12), %xmm14\n\t"
" addpd %xmm11, %xmm7\n\t"
" movapd %xmm10, %xmm11\n\t"
" pshufd $ 0x4e, %xmm10, %xmm15\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 1, %r10d\n\t"
"\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 32, %r12\n\t"
"\n\t"
" addpd %xmm10, %xmm1\n\t"
" addpd %xmm11, %xmm5\n\t"
" movapd %xmm14, %xmm11\n\t"
" pshufd $ 0x4e, %xmm14, %xmm12\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addq $ 32, %r11\n\t"
"\n\t"
" addpd %xmm15, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jg 3b\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm3\n\t"
" addpd %xmm11, %xmm7\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
"\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size inner_kernel_dgemm_nt_4x4_lib4, .-inner_kernel_dgemm_nt_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .type inner_kernel_dgemm_nn_4x4_lib4, @function; inner_kernel_dgemm_nn_4x4_lib4:\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" xorpd %xmm11, %xmm11\n\t"
" movapd %xmm11, %xmm12\n\t"
" movapd %xmm11, %xmm13\n\t"
" movapd %xmm11, %xmm14\n\t"
" movapd %xmm11, %xmm15\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
"\n\t"
" .p2align 3\n\t"
"1:\n\t"
"\n\t"
" prefetcht0 0(%r12, %r13, 2)\n\t"
" prefetcht0 64(%r12, %r13, 2)\n\t"
"\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 32(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 72(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 104(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 64(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 112(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 96(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 24(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 56(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 128, %r11\n\t"
"\n\t"
" movddup 88(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 120(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 0(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 16(%r11), %xmm9\n\t"
" addq %r13, %r12\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jg 1b\n\t"
"\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" cmpl $ 3, %r10d\n\t"
" jle 4f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 32(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 72(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 104(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 64(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 112(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" movapd 96(%r11), %xmm8\n\t"
" mulpd %xmm9, %xmm13\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 24(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 56(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 128, %r11\n\t"
"\n\t"
" movddup 88(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
"\n\t"
" movddup 120(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
"\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" addq %r13, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
" jmp 2f\n\t"
"\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" subl $ 1, %r10d\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addq $ 32, %r11\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jg 3b\n\t"
"\n\t"
"\n\t"
"\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size inner_kernel_dgemm_nn_4x4_lib4, .-inner_kernel_dgemm_nn_4x4_lib4\n\t"
" .macro INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r14d\n\t"
" jle 2f\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
" movl $ 4, %r15d\n\t"
" subl %r14d, %r15d\n\t"
" cmpl %r10d, %r15d\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmovgl %r10d, %r15d\n\t"
"\n\t"
" movl %r14d, %eax\n\t"
" sall $ 3, %eax\n\t"
" addq %rax, %r12\n\t"
"\n\t"
"1:\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" subl $ 1, %r15d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r15d\n\t"
" jg 1b\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
" addq %r13, %r12\n\t"
" subq $ 32, %r12\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRMM_NN_RL_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r14d\n\t"
" jg 0f\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
"\n\t"
" movapd 32(%r11), %xmm8\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
"\n\t"
" movapd 64(%r11), %xmm8\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
"\n\t"
" movapd 96(%r11), %xmm8\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
" movddup 24(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 56(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 88(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 120(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 4, %r10d\n\t"
" addq $ 128, %r11\n\t"
" addq %r13, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
" cmpl $ 1, %r14d\n\t"
" jg 1f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
"\n\t"
" movapd 32(%r11), %xmm8\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
"\n\t"
" movapd 64(%r11), %xmm8\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" subl $ 3, %r10d\n\t"
" addq $ 96, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 8, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addq $ 16, %r12\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
"\n\t"
" movapd 32(%r11), %xmm8\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" subl $ 2, %r10d\n\t"
" addq $ 64, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 16, %r12\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
"\n\t"
" movapd 32(%r11), %xmm8\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 72(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 104(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
" movapd 64(%r11), %xmm8\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 112(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
" movapd 96(%r11), %xmm8\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
" movddup 24(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 56(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 88(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 120(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 4, %r10d\n\t"
" addq $ 128, %r11\n\t"
" addq %r13, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
" addq $ 24, %r12\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
"\n\t"
" movapd 32(%r11), %xmm8\n\t"
" movapd 48(%r11), %xmm9\n\t"
"\n\t"
" movddup 8(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 40(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 72(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
"\n\t"
" movapd 64(%r11), %xmm8\n\t"
" movapd 80(%r11), %xmm9\n\t"
"\n\t"
" movddup 16(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 48(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 80(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 112(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
" movapd 96(%r11), %xmm8\n\t"
" movapd 112(%r11), %xmm9\n\t"
"\n\t"
" movddup 24(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 56(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 88(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 120(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 4, %r10d\n\t"
" addq $ 128, %r11\n\t"
" addq %r13, %r12\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRMM_NN_RL_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" cmpl $ 0, %r14d\n\t"
" jg 0f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
" cmpl $ 1, %r14d\n\t"
" jg 1f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addq $ 16, %r12\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
" addq $ 24, %r12\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq $ 8, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 3f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
"\n\t"
" movddup 0(%r12), %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
" mulpd %xmm8, %xmm10\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm10, %xmm0\n\t"
" addpd %xmm11, %xmm4\n\t"
"\n\t"
" movddup 32(%r12), %xmm15\n\t"
" movapd %xmm15, %xmm13\n\t"
" mulpd %xmm8, %xmm15\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm15, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movddup 64(%r12), %xmm14\n\t"
" movapd %xmm14, %xmm11\n\t"
" mulpd %xmm8, %xmm14\n\t"
" mulpd %xmm9, %xmm11\n\t"
" addpd %xmm14, %xmm2\n\t"
" addpd %xmm11, %xmm6\n\t"
"\n\t"
" movddup 96(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 32, %r11\n\t"
" addq %r13, %r12\n\t"
" subq $ 24, %r12\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRMM_NT_RU_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r10), %xmm8\n\t"
" movapd 16(%r10), %xmm9\n\t"
" movddup 0(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
"\n\t"
" movapd 32(%r10), %xmm8\n\t"
" movapd 48(%r10), %xmm9\n\t"
" movddup 32(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movddup 40(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
"\n\t"
" movapd 64(%r10), %xmm8\n\t"
" movapd 80(%r10), %xmm9\n\t"
" movddup 64(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movddup 72(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movddup 80(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
"\n\t"
" movapd 96(%r10), %xmm8\n\t"
" movapd 112(%r10), %xmm9\n\t"
" movddup 96(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movddup 104(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movddup 112(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" movddup 120(%r11), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
"\n\t"
" addq $ 128, %r10\n\t"
" addq $ 128, %r11\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRMM_NT_RU_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" subl $ 1, %r10d\n\t"
" movddup 0(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" addq $ 32, %r11\n\t"
" addq $ 32, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" subl $ 1, %r10d\n\t"
" movddup 0(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" addq $ 32, %r11\n\t"
" movddup 8(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" addq $ 32, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" subl $ 1, %r10d\n\t"
" movddup 0(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movddup 8(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" addq $ 32, %r11\n\t"
" movddup 16(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" addq $ 32, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd 0(%r11), %xmm8\n\t"
" movapd 16(%r11), %xmm9\n\t"
" subl $ 1, %r10d\n\t"
" movddup 0(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm0\n\t"
" addpd %xmm13, %xmm4\n\t"
" movddup 8(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm1\n\t"
" addpd %xmm13, %xmm5\n\t"
" movddup 16(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm2\n\t"
" addpd %xmm13, %xmm6\n\t"
" addq $ 32, %r11\n\t"
" movddup 24(%r12), %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm8, %xmm12\n\t"
" mulpd %xmm9, %xmm13\n\t"
" addpd %xmm12, %xmm3\n\t"
" addpd %xmm13, %xmm7\n\t"
" addq $ 32, %r12\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm8\n\t"
" movsd %xmm1, %xmm0\n\t"
" movsd %xmm8, %xmm1\n\t"
"\n\t"
" movapd %xmm2, %xmm8\n\t"
" movsd %xmm3, %xmm2\n\t"
" movsd %xmm8, %xmm3\n\t"
"\n\t"
" movapd %xmm4, %xmm8\n\t"
" movsd %xmm5, %xmm4\n\t"
" movsd %xmm8, %xmm5\n\t"
"\n\t"
" movapd %xmm6, %xmm8\n\t"
" movsd %xmm7, %xmm6\n\t"
" movsd %xmm8, %xmm7\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm15\n\t"
"\n\t"
" mulpd %xmm15, %xmm0\n\t"
" mulpd %xmm15, %xmm1\n\t"
" mulpd %xmm15, %xmm2\n\t"
" mulpd %xmm15, %xmm3\n\t"
" mulpd %xmm15, %xmm4\n\t"
" mulpd %xmm15, %xmm5\n\t"
" mulpd %xmm15, %xmm6\n\t"
" mulpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm14\n\t"
"\n\t"
" movapd 0(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 16(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 32(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 48(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 64(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 80(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 96(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 112(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_SCALE_AB_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm15\n\t"
"\n\t"
" mulpd %xmm15, %xmm0\n\t"
" mulpd %xmm15, %xmm1\n\t"
" mulpd %xmm15, %xmm2\n\t"
" mulpd %xmm15, %xmm3\n\t"
" mulpd %xmm15, %xmm4\n\t"
" mulpd %xmm15, %xmm5\n\t"
" mulpd %xmm15, %xmm6\n\t"
" mulpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm14\n\t"
"\n\t"
" xorpd %xmm15, %xmm15\n\t"
"\n\t"
" ucomisd %xmm14, %xmm15\n\t"
" je 3f\n\t"
"\n\t"
" cmpl $ 0, %r12d\n\t"
" jg 0f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 16(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 32(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 48(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 64(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 80(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 96(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 112(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" movq %r13, %r15\n\t"
" addq %r14, %r15\n\t"
"\n\t"
" cmpl $ 1, %r12d\n\t"
" jg 1f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movupd 8(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movlpd 24(%r13), %xmm15\n\t"
" movhpd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movupd 40(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movlpd 56(%r13), %xmm15\n\t"
" movhpd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movupd 72(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movlpd 88(%r13), %xmm15\n\t"
" movhpd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movupd 104(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movlpd 120(%r13), %xmm15\n\t"
" movhpd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
"\n\t"
" cmpl $ 2, %r12d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 16(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 48(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 80(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 112(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movlpd 24(%r13), %xmm15\n\t"
" movhpd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movupd 8(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movlpd 56(%r13), %xmm15\n\t"
" movhpd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movupd 40(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movlpd 88(%r13), %xmm15\n\t"
" movhpd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movupd 72(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movlpd 120(%r13), %xmm15\n\t"
" movhpd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movupd 104(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm15\n\t"
"\n\t"
" mulpd %xmm15, %xmm0\n\t"
" mulpd %xmm15, %xmm1\n\t"
" mulpd %xmm15, %xmm2\n\t"
" mulpd %xmm15, %xmm3\n\t"
" mulpd %xmm15, %xmm4\n\t"
" mulpd %xmm15, %xmm5\n\t"
" mulpd %xmm15, %xmm6\n\t"
" mulpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm8\n\t"
" movsd %xmm1, %xmm0\n\t"
" movsd %xmm8, %xmm1\n\t"
"\n\t"
" movapd %xmm2, %xmm8\n\t"
" movsd %xmm3, %xmm2\n\t"
" movsd %xmm8, %xmm3\n\t"
"\n\t"
" movapd %xmm4, %xmm8\n\t"
" movsd %xmm5, %xmm4\n\t"
" movsd %xmm8, %xmm5\n\t"
"\n\t"
" movapd %xmm6, %xmm8\n\t"
" movsd %xmm7, %xmm6\n\t"
" movsd %xmm8, %xmm7\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm15\n\t"
"\n\t"
" mulpd %xmm15, %xmm0\n\t"
" mulpd %xmm15, %xmm1\n\t"
" mulpd %xmm15, %xmm2\n\t"
" mulpd %xmm15, %xmm3\n\t"
" mulpd %xmm15, %xmm4\n\t"
" mulpd %xmm15, %xmm5\n\t"
" mulpd %xmm15, %xmm6\n\t"
" mulpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm14\n\t"
"\n\t"
" xorpd %xmm15, %xmm15\n\t"
" ucomisd %xmm15, %xmm14\n\t"
" je 0f\n\t"
"\n\t"
" movapd 0(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 16(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 32(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 48(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 64(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 80(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 96(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 112(%r12), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_SCALE_AB_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm8\n\t"
" movsd %xmm1, %xmm0\n\t"
" movsd %xmm8, %xmm1\n\t"
"\n\t"
" movapd %xmm2, %xmm8\n\t"
" movsd %xmm3, %xmm2\n\t"
" movsd %xmm8, %xmm3\n\t"
"\n\t"
" movapd %xmm4, %xmm8\n\t"
" movsd %xmm5, %xmm4\n\t"
" movsd %xmm8, %xmm5\n\t"
"\n\t"
" movapd %xmm6, %xmm8\n\t"
" movsd %xmm7, %xmm6\n\t"
" movsd %xmm8, %xmm7\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm15\n\t"
"\n\t"
" mulpd %xmm15, %xmm0\n\t"
" mulpd %xmm15, %xmm1\n\t"
" mulpd %xmm15, %xmm2\n\t"
" mulpd %xmm15, %xmm3\n\t"
" mulpd %xmm15, %xmm4\n\t"
" mulpd %xmm15, %xmm5\n\t"
" mulpd %xmm15, %xmm6\n\t"
" mulpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm14\n\t"
"\n\t"
" xorpd %xmm15, %xmm15\n\t"
"\n\t"
" ucomisd %xmm14, %xmm15\n\t"
" je 3f\n\t"
"\n\t"
" cmpl $ 0, %r12d\n\t"
" jg 0f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 0(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 16(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 32(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 48(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 64(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 80(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 96(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 112(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" movq %r13, %r15\n\t"
" addq %r14, %r15\n\t"
"\n\t"
" cmpl $ 1, %r12d\n\t"
" jg 1f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movupd 8(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movlpd 24(%r13), %xmm15\n\t"
" movhpd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movupd 40(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movlpd 56(%r13), %xmm15\n\t"
" movhpd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movupd 72(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movlpd 88(%r13), %xmm15\n\t"
" movhpd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movupd 104(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movlpd 120(%r13), %xmm15\n\t"
" movhpd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
"\n\t"
" cmpl $ 2, %r12d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd 16(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movapd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movapd 48(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movapd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movapd 80(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movapd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movapd 112(%r13), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movapd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movlpd 24(%r13), %xmm15\n\t"
" movhpd 0(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm0\n\t"
" movupd 8(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm4\n\t"
" movlpd 56(%r13), %xmm15\n\t"
" movhpd 32(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm1\n\t"
" movupd 40(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm5\n\t"
" movlpd 88(%r13), %xmm15\n\t"
" movhpd 64(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm2\n\t"
" movupd 72(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm6\n\t"
" movlpd 120(%r13), %xmm15\n\t"
" movhpd 96(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm3\n\t"
" movupd 104(%r15), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" addpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_SCALE_M1B_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm8\n\t"
" movsd %xmm1, %xmm0\n\t"
" movsd %xmm8, %xmm1\n\t"
"\n\t"
" movapd %xmm2, %xmm8\n\t"
" movsd %xmm3, %xmm2\n\t"
" movsd %xmm8, %xmm3\n\t"
"\n\t"
" movapd %xmm4, %xmm8\n\t"
" movsd %xmm5, %xmm4\n\t"
" movsd %xmm8, %xmm5\n\t"
"\n\t"
" movapd %xmm6, %xmm8\n\t"
" movsd %xmm7, %xmm6\n\t"
" movsd %xmm8, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r10), %xmm14\n\t"
"\n\t"
" xorpd %xmm15, %xmm15\n\t"
" ucomisd %xmm15, %xmm14\n\t"
" je 0f\n\t"
"\n\t"
" movapd 0(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm0, %xmm15\n\t"
" movapd %xmm15, %xmm0\n\t"
" movapd 16(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm4, %xmm15\n\t"
" movapd %xmm15, %xmm4\n\t"
" movapd 32(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm1, %xmm15\n\t"
" movapd %xmm15, %xmm1\n\t"
" movapd 48(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm5, %xmm15\n\t"
" movapd %xmm15, %xmm5\n\t"
" movapd 64(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm2, %xmm15\n\t"
" movapd %xmm15, %xmm2\n\t"
" movapd 80(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm6, %xmm15\n\t"
" movapd %xmm15, %xmm6\n\t"
" movapd 96(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm3, %xmm15\n\t"
" movapd %xmm15, %xmm3\n\t"
" movapd 112(%r11), %xmm15\n\t"
" mulpd %xmm14, %xmm15\n\t"
" subpd %xmm7, %xmm15\n\t"
" movapd %xmm15, %xmm7\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm8\n\t"
" movsd %xmm1, %xmm0\n\t"
" movsd %xmm8, %xmm1\n\t"
"\n\t"
" movapd %xmm2, %xmm8\n\t"
" movsd %xmm3, %xmm2\n\t"
" movsd %xmm8, %xmm3\n\t"
"\n\t"
" movapd %xmm4, %xmm8\n\t"
" movsd %xmm5, %xmm4\n\t"
" movsd %xmm8, %xmm5\n\t"
"\n\t"
" movapd %xmm6, %xmm8\n\t"
" movsd %xmm7, %xmm6\n\t"
" movsd %xmm8, %xmm7\n\t"
"\n\t"
"\n\t"
" movapd 0(%r10), %xmm15\n\t"
" subpd %xmm0, %xmm15\n\t"
" movapd %xmm15, %xmm0\n\t"
" movapd 16(%r10), %xmm15\n\t"
" subpd %xmm4, %xmm15\n\t"
" movapd %xmm15, %xmm4\n\t"
" movapd 32(%r10), %xmm15\n\t"
" subpd %xmm1, %xmm15\n\t"
" movapd %xmm15, %xmm1\n\t"
" movapd 48(%r10), %xmm15\n\t"
" subpd %xmm5, %xmm15\n\t"
" movapd %xmm15, %xmm5\n\t"
" movapd 64(%r10), %xmm15\n\t"
" subpd %xmm2, %xmm15\n\t"
" movapd %xmm15, %xmm2\n\t"
" movapd 80(%r10), %xmm15\n\t"
" subpd %xmm6, %xmm15\n\t"
" movapd %xmm15, %xmm6\n\t"
" movapd 96(%r10), %xmm15\n\t"
" subpd %xmm3, %xmm15\n\t"
" movapd %xmm15, %xmm3\n\t"
" movapd 112(%r10), %xmm15\n\t"
" subpd %xmm7, %xmm15\n\t"
" movapd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DPOTRF_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm15, %xmm15\n\t"
"\n\t"
" movsd %xmm0, %xmm13\n\t"
" ucomisd %xmm15, %xmm13\n\t"
" jbe 1f\n\t"
" sqrtsd %xmm13, %xmm13\n\t"
"\n\t"
" movsd .LC04(%rip), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" divsd %xmm13, %xmm12\n\t"
"2:\n\t"
" cmpl $ 2, %r11d\n\t"
" movsd %xmm12, 0(%r10)\n\t"
" movddup %xmm12, %xmm12\n\t"
" mulpd %xmm12, %xmm0\n\t"
" mulpd %xmm12, %xmm4\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movapd %xmm0, %xmm12\n\t"
" shufpd $ 0x3, %xmm12, %xmm12\n\t"
" movapd %xmm12, %xmm13\n\t"
" mulpd %xmm0, %xmm12\n\t"
" mulpd %xmm4, %xmm13\n\t"
" subpd %xmm12, %xmm1\n\t"
" subpd %xmm13, %xmm5\n\t"
" movapd %xmm1, %xmm13\n\t"
" shufpd $ 0x3, %xmm13, %xmm13\n\t"
" ucomisd %xmm15, %xmm13\n\t"
" jbe 3f\n\t"
" sqrtsd %xmm13, %xmm13\n\t"
"\n\t"
" movsd .LC04(%rip), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" divsd %xmm13, %xmm12\n\t"
"4:\n\t"
" cmpl $ 3, %r11d\n\t"
" movsd %xmm12, 8(%r10)\n\t"
" movddup %xmm12, %xmm12\n\t"
" mulpd %xmm12, %xmm1\n\t"
" mulpd %xmm12, %xmm5\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movddup %xmm4, %xmm12\n\t"
" movddup %xmm5, %xmm13\n\t"
" mulpd %xmm4, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" subpd %xmm12, %xmm6\n\t"
" subpd %xmm13, %xmm6\n\t"
" movsd %xmm6, %xmm13\n\t"
" ucomisd %xmm15, %xmm13\n\t"
" jbe 5f\n\t"
" sqrtsd %xmm13, %xmm13\n\t"
"\n\t"
" movsd .LC04(%rip), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" divsd %xmm13, %xmm12\n\t"
"6:\n\t"
" cmpl $ 4, %r11d\n\t"
" movsd %xmm12, 16(%r10)\n\t"
" movddup %xmm12, %xmm12\n\t"
" mulpd %xmm12, %xmm6\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movapd %xmm4, %xmm12\n\t"
" movapd %xmm5, %xmm13\n\t"
" movapd %xmm6, %xmm14\n\t"
" shufpd $ 0x3, %xmm12, %xmm12\n\t"
" shufpd $ 0x3, %xmm13, %xmm13\n\t"
" shufpd $ 0x3, %xmm14, %xmm14\n\t"
" mulpd %xmm4, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" mulpd %xmm6, %xmm14\n\t"
" subpd %xmm12, %xmm7\n\t"
" subpd %xmm13, %xmm7\n\t"
" subpd %xmm14, %xmm7\n\t"
" movapd %xmm7, %xmm13\n\t"
" shufpd $ 0x3, %xmm13, %xmm13\n\t"
" ucomisd %xmm15, %xmm13\n\t"
" jbe 7f\n\t"
" sqrtsd %xmm13, %xmm13\n\t"
"\n\t"
" movsd .LC04(%rip), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" divsd %xmm13, %xmm12\n\t"
"8:\n\t"
" movsd %xmm12, 24(%r10)\n\t"
" movddup %xmm12, %xmm12\n\t"
" mulpd %xmm12, %xmm7\n\t"
"\n\t"
" jmp 0f\n\t"
"\n\t"
"1:\n\t"
" xorpd %xmm12, %xmm12\n\t"
" jmp 2b\n\t"
"\n\t"
"3:\n\t"
" xorpd %xmm12, %xmm12\n\t"
" jmp 4b\n\t"
"\n\t"
"5:\n\t"
" xorpd %xmm12, %xmm12\n\t"
" jmp 6b\n\t"
"\n\t"
"7:\n\t"
" xorpd %xmm12, %xmm12\n\t"
" jmp 8b\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRSM_RLT_INV_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm0\n\t"
" mulpd %xmm13, %xmm4\n\t"
" movddup 8(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm13\n\t"
" mulpd %xmm4, %xmm12\n\t"
" subpd %xmm13, %xmm1\n\t"
" subpd %xmm12, %xmm5\n\t"
" movddup 16(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm12\n\t"
" mulpd %xmm4, %xmm13\n\t"
" subpd %xmm12, %xmm2\n\t"
" subpd %xmm13, %xmm6\n\t"
" movddup 24(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm12\n\t"
" mulpd %xmm4, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
"\n\t"
" movddup 8(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm1\n\t"
" mulpd %xmm13, %xmm5\n\t"
" movddup 48(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm1, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" subpd %xmm12, %xmm2\n\t"
" subpd %xmm13, %xmm6\n\t"
" movddup 56(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm1, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
"\n\t"
" movddup 16(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm2\n\t"
" mulpd %xmm13, %xmm6\n\t"
" movddup 88(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm2, %xmm12\n\t"
" mulpd %xmm6, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
"\n\t"
" movddup 24(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm3\n\t"
" mulpd %xmm13, %xmm7\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_EDGE_DTRSM_RLT_INV_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movddup 0(%r11), %xmm13\n\t"
" cmpl $ 2, %r12d\n\t"
" mulpd %xmm13, %xmm0\n\t"
" mulpd %xmm13, %xmm4\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movddup 8(%r10), %xmm13\n\t"
" cmpl $ 3, %r12d\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm13\n\t"
" mulpd %xmm4, %xmm12\n\t"
" subpd %xmm13, %xmm1\n\t"
" subpd %xmm12, %xmm5\n\t"
" movddup 8(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm1\n\t"
" mulpd %xmm13, %xmm5\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movddup 16(%r10), %xmm13\n\t"
" cmpl $ 4, %r12d\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm12\n\t"
" mulpd %xmm4, %xmm13\n\t"
" subpd %xmm12, %xmm2\n\t"
" subpd %xmm13, %xmm6\n\t"
" movddup 48(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm1, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" subpd %xmm12, %xmm2\n\t"
" subpd %xmm13, %xmm6\n\t"
" movddup 16(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm2\n\t"
" mulpd %xmm13, %xmm6\n\t"
"\n\t"
" jl 0f\n\t"
"\n\t"
" movddup 24(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm0, %xmm12\n\t"
" mulpd %xmm4, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
" movddup 56(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm1, %xmm12\n\t"
" mulpd %xmm5, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
" movddup 88(%r10), %xmm13\n\t"
" movapd %xmm13, %xmm12\n\t"
" mulpd %xmm2, %xmm12\n\t"
" mulpd %xmm6, %xmm13\n\t"
" subpd %xmm12, %xmm3\n\t"
" subpd %xmm13, %xmm7\n\t"
" movddup 24(%r11), %xmm13\n\t"
" mulpd %xmm13, %xmm3\n\t"
" mulpd %xmm13, %xmm7\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movapd %xmm4, 16(%r10)\n\t"
" movapd %xmm1, 32(%r10)\n\t"
" movapd %xmm5, 48(%r10)\n\t"
" movapd %xmm2, 64(%r10)\n\t"
" movapd %xmm6, 80(%r10)\n\t"
" movapd %xmm3, 96(%r10)\n\t"
" movapd %xmm7, 112(%r10)\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 2, %r11d\n\t"
" jg 1f\n\t"
" je 0f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 0(%r10)\n\t"
" cmpl $ 2, %r12d\n\t"
" jl 4f\n\t"
" movsd %xmm1, 32(%r10)\n\t"
" cmpl $ 3, %r12d\n\t"
" jl 4f\n\t"
" movsd %xmm2, 64(%r10)\n\t"
" je 4f\n\t"
" movsd %xmm3, 96(%r10)\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" cmpl $ 2, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm1, 32(%r10)\n\t"
" cmpl $ 3, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm2, 64(%r10)\n\t"
" je 4f\n\t"
" movapd %xmm3, 96(%r10)\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"1:\n\t"
" cmpl $ 3, %r11d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movsd %xmm4, 16(%r10)\n\t"
" cmpl $ 2, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm1, 32(%r10)\n\t"
" movsd %xmm5, 48(%r10)\n\t"
" cmpl $ 3, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm2, 64(%r10)\n\t"
" movsd %xmm6, 80(%r10)\n\t"
" je 4f\n\t"
" movapd %xmm3, 96(%r10)\n\t"
" movsd %xmm7, 112(%r10)\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movapd %xmm4, 16(%r10)\n\t"
" cmpl $ 2, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm1, 32(%r10)\n\t"
" movapd %xmm5, 48(%r10)\n\t"
" cmpl $ 3, %r12d\n\t"
" jl 4f\n\t"
" movapd %xmm2, 64(%r10)\n\t"
" movapd %xmm6, 80(%r10)\n\t"
" je 4f\n\t"
" movapd %xmm3, 96(%r10)\n\t"
" movapd %xmm7, 112(%r10)\n\t"
"\n\t"
"4:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_4X4_GEN_LIB4\n\t"
" cmpl $ 0, %r15d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd %xmm1, %xmm0\n\t"
" movapd %xmm5, %xmm4\n\t"
" movapd %xmm2, %xmm1\n\t"
" movapd %xmm6, %xmm5\n\t"
" movapd %xmm3, %xmm2\n\t"
" movapd %xmm7, %xmm6\n\t"
" addq $ 32, %r11\n\t"
"\n\t"
" cmpl $ 1, %r15d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd %xmm1, %xmm0\n\t"
" movapd %xmm5, %xmm4\n\t"
" movapd %xmm2, %xmm1\n\t"
" movapd %xmm6, %xmm5\n\t"
" addq $ 32, %r11\n\t"
"\n\t"
" cmpl $ 2, %r15d\n\t"
" jle 0f\n\t"
"\n\t"
" movapd %xmm1, %xmm0\n\t"
" movapd %xmm5, %xmm4\n\t"
" addq $ 32, %r11\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %eax\n\t"
" jle 0f\n\t"
" movl $ 4, %eax\n\t"
"0:\n\t"
" subl %r15d, %eax\n\t"
" movl %eax, %r15d\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jg 0f\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r13d\n\t"
" jle 4f\n\t"
"\n\t"
" cmpl $ 1, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 0(%r11), %xmm0\n\t"
" movlpd 32(%r11), %xmm1\n\t"
" movlpd 64(%r11), %xmm2\n\t"
" movlpd 96(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 2, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movapd 0(%r11), %xmm0\n\t"
" movapd 32(%r11), %xmm1\n\t"
" movapd 64(%r11), %xmm2\n\t"
" movapd 96(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 3, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movapd 0(%r11), %xmm0\n\t"
" movlpd 16(%r11), %xmm4\n\t"
" movapd 32(%r11), %xmm1\n\t"
" movlpd 48(%r11), %xmm5\n\t"
" movapd 64(%r11), %xmm2\n\t"
" movlpd 80(%r11), %xmm6\n\t"
" movapd 96(%r11), %xmm3\n\t"
" movlpd 112(%r11), %xmm7\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
"\n\t"
" movapd 0(%r11), %xmm0\n\t"
" movapd 16(%r11), %xmm4\n\t"
" movapd 32(%r11), %xmm1\n\t"
" movapd 48(%r11), %xmm5\n\t"
" movapd 64(%r11), %xmm2\n\t"
" movapd 80(%r11), %xmm6\n\t"
" movapd 96(%r11), %xmm3\n\t"
" movapd 112(%r11), %xmm7\n\t"
"\n\t"
"4:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 5f\n\t"
" je 4f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 0(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 32(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 64(%r11)\n\t"
" je 3f\n\t"
" movsd %xmm3, 96(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" movapd %xmm0, 0(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 32(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 64(%r11)\n\t"
" je 3f\n\t"
" movapd %xmm3, 96(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"5:\n\t"
" cmpl $ 3, %r14d\n\t"
" jg 6f\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 0(%r11)\n\t"
" movsd %xmm4, 16(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 32(%r11)\n\t"
" movsd %xmm5, 48(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 64(%r11)\n\t"
" movsd %xmm6, 80(%r11)\n\t"
" je 3f\n\t"
" movapd %xmm3, 96(%r11)\n\t"
" movsd %xmm7, 112(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"6:\n\t"
"\n\t"
" movapd %xmm0, 0(%r11)\n\t"
" movapd %xmm4, 16(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 32(%r11)\n\t"
" movapd %xmm5, 48(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 64(%r11)\n\t"
" movapd %xmm6, 80(%r11)\n\t"
" je 3f\n\t"
" movapd %xmm3, 96(%r11)\n\t"
" movapd %xmm7, 112(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" movq %r11, %rbx\n\t"
" addq %r12, %rbx\n\t"
"\n\t"
" cmpl $ 1, %r10d\n\t"
" jg 1f\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r13d\n\t"
" jle 4f\n\t"
"\n\t"
" cmpl $ 1, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 8(%r11), %xmm0\n\t"
" movlpd 40(%r11), %xmm1\n\t"
" movlpd 72(%r11), %xmm2\n\t"
" movlpd 104(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 2, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movupd 8(%r11), %xmm0\n\t"
" movupd 40(%r11), %xmm1\n\t"
" movupd 72(%r11), %xmm2\n\t"
" movupd 104(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 3, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movupd 8(%r11), %xmm0\n\t"
" movupd 40(%r11), %xmm1\n\t"
" movupd 72(%r11), %xmm2\n\t"
" movupd 104(%r11), %xmm3\n\t"
" movlpd 24(%r11), %xmm4\n\t"
" movlpd 56(%r11), %xmm5\n\t"
" movlpd 88(%r11), %xmm6\n\t"
" movlpd 120(%r11), %xmm7\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
"\n\t"
" movupd 8(%r11), %xmm0\n\t"
" movupd 40(%r11), %xmm1\n\t"
" movupd 72(%r11), %xmm2\n\t"
" movupd 104(%r11), %xmm3\n\t"
" movlpd 24(%r11), %xmm4\n\t"
" movlpd 56(%r11), %xmm5\n\t"
" movlpd 88(%r11), %xmm6\n\t"
" movlpd 120(%r11), %xmm7\n\t"
" movhpd 0(%rbx), %xmm4\n\t"
" movhpd 32(%rbx), %xmm5\n\t"
" movhpd 64(%rbx), %xmm6\n\t"
" movhpd 96(%rbx), %xmm7\n\t"
"\n\t"
"4:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 5f\n\t"
" je 4f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 8(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 40(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 72(%r11)\n\t"
" je 3f\n\t"
" movsd %xmm3, 104(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" movupd %xmm0, 8(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm1, 40(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm2, 72(%r11)\n\t"
" je 3f\n\t"
" movupd %xmm3, 104(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"5:\n\t"
" cmpl $ 3, %r14d\n\t"
" jg 6f\n\t"
"\n\t"
"\n\t"
" movupd %xmm0, 8(%r11)\n\t"
" movsd %xmm4, 24(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm1, 40(%r11)\n\t"
" movsd %xmm5, 56(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm2, 72(%r11)\n\t"
" movsd %xmm6, 88(%r11)\n\t"
" je 3f\n\t"
" movupd %xmm3, 104(%r11)\n\t"
" movsd %xmm7, 120(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"6:\n\t"
"\n\t"
" movupd %xmm0, 8(%r11)\n\t"
" movsd %xmm4, 24(%r11)\n\t"
" movhpd %xmm4, 0(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm1, 40(%r11)\n\t"
" movsd %xmm5, 56(%r11)\n\t"
" movhpd %xmm5, 32(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movupd %xmm2, 72(%r11)\n\t"
" movsd %xmm6, 88(%r11)\n\t"
" movhpd %xmm6, 64(%rbx)\n\t"
" je 3f\n\t"
" movupd %xmm3, 104(%r11)\n\t"
" movsd %xmm7, 120(%r11)\n\t"
" movhpd %xmm7, 96(%rbx)\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
"\n\t"
" cmpl $ 2, %r10d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r13d\n\t"
" jle 4f\n\t"
"\n\t"
" cmpl $ 1, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 16(%r11), %xmm0\n\t"
" movlpd 48(%r11), %xmm1\n\t"
" movlpd 80(%r11), %xmm2\n\t"
" movlpd 112(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 2, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movapd 16(%r11), %xmm0\n\t"
" movapd 48(%r11), %xmm1\n\t"
" movapd 80(%r11), %xmm2\n\t"
" movapd 112(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 3, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movapd 16(%r11), %xmm0\n\t"
" movlpd 0(%rbx), %xmm4\n\t"
" movapd 48(%r11), %xmm1\n\t"
" movlpd 32(%rbx), %xmm5\n\t"
" movapd 80(%r11), %xmm2\n\t"
" movlpd 64(%rbx), %xmm6\n\t"
" movapd 112(%r11), %xmm3\n\t"
" movlpd 96(%rbx), %xmm7\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
"\n\t"
" movapd 16(%r11), %xmm0\n\t"
" movapd 0(%rbx), %xmm4\n\t"
" movapd 48(%r11), %xmm1\n\t"
" movapd 32(%rbx), %xmm5\n\t"
" movapd 80(%r11), %xmm2\n\t"
" movapd 64(%rbx), %xmm6\n\t"
" movapd 112(%r11), %xmm3\n\t"
" movapd 96(%rbx), %xmm7\n\t"
"\n\t"
"4:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 5f\n\t"
" je 4f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 16(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 48(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 80(%r11)\n\t"
" je 3f\n\t"
" movsd %xmm3, 112(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" movapd %xmm0, 16(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 48(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 80(%r11)\n\t"
" je 3f\n\t"
" movapd %xmm3, 112(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"5:\n\t"
" cmpl $ 3, %r14d\n\t"
" jg 6f\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 16(%r11)\n\t"
" movsd %xmm4, 0(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 48(%r11)\n\t"
" movsd %xmm5, 32(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 80(%r11)\n\t"
" movsd %xmm6, 64(%rbx)\n\t"
" je 3f\n\t"
" movapd %xmm3, 112(%r11)\n\t"
" movsd %xmm7, 96(%rbx)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"6:\n\t"
"\n\t"
" movapd %xmm0, 16(%r11)\n\t"
" movapd %xmm4, 0(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm1, 48(%r11)\n\t"
" movapd %xmm5, 32(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movapd %xmm2, 80(%r11)\n\t"
" movapd %xmm6, 64(%rbx)\n\t"
" je 3f\n\t"
" movapd %xmm3, 112(%r11)\n\t"
" movapd %xmm7, 96(%rbx)\n\t"
"\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r13d\n\t"
" jle 4f\n\t"
"\n\t"
" cmpl $ 1, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 24(%r11), %xmm0\n\t"
" movlpd 56(%r11), %xmm1\n\t"
" movlpd 88(%r11), %xmm2\n\t"
" movlpd 120(%r11), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 2, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 24(%r11), %xmm0\n\t"
" movlpd 56(%r11), %xmm1\n\t"
" movlpd 88(%r11), %xmm2\n\t"
" movlpd 120(%r11), %xmm3\n\t"
" movhpd 0(%rbx), %xmm0\n\t"
" movhpd 32(%rbx), %xmm1\n\t"
" movhpd 64(%rbx), %xmm2\n\t"
" movhpd 96(%rbx), %xmm3\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" cmpl $ 3, %r13d\n\t"
" jg 5f\n\t"
"\n\t"
" movlpd 24(%r11), %xmm0\n\t"
" movlpd 56(%r11), %xmm1\n\t"
" movlpd 88(%r11), %xmm2\n\t"
" movlpd 120(%r11), %xmm3\n\t"
" movhpd 0(%rbx), %xmm0\n\t"
" movhpd 32(%rbx), %xmm1\n\t"
" movhpd 64(%rbx), %xmm2\n\t"
" movhpd 96(%rbx), %xmm3\n\t"
" movlpd 8(%rbx), %xmm4\n\t"
" movlpd 40(%rbx), %xmm5\n\t"
" movlpd 72(%rbx), %xmm6\n\t"
" movlpd 104(%rbx), %xmm7\n\t"
"\n\t"
" jmp 4f\n\t"
"\n\t"
"5:\n\t"
"\n\t"
" movlpd 24(%r11), %xmm0\n\t"
" movlpd 56(%r11), %xmm1\n\t"
" movlpd 88(%r11), %xmm2\n\t"
" movlpd 120(%r11), %xmm3\n\t"
" movhpd 0(%rbx), %xmm0\n\t"
" movhpd 32(%rbx), %xmm1\n\t"
" movhpd 64(%rbx), %xmm2\n\t"
" movhpd 96(%rbx), %xmm3\n\t"
" movupd 8(%rbx), %xmm4\n\t"
" movupd 40(%rbx), %xmm5\n\t"
" movupd 72(%rbx), %xmm6\n\t"
" movupd 104(%rbx), %xmm7\n\t"
"\n\t"
"4:\n\t"
" cmpl $ 2, %r14d\n\t"
" jg 5f\n\t"
" je 4f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 24(%r11)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 56(%r11)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 88(%r11)\n\t"
" je 3f\n\t"
" movsd %xmm3, 120(%r11)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" movsd %xmm0, 24(%r11)\n\t"
" movhpd %xmm0, 0(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 56(%r11)\n\t"
" movhpd %xmm1, 32(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 88(%r11)\n\t"
" movhpd %xmm2, 64(%rbx)\n\t"
" je 3f\n\t"
" movsd %xmm3, 120(%r11)\n\t"
" movhpd %xmm3, 96(%rbx)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"5:\n\t"
" cmpl $ 3, %r14d\n\t"
" jg 6f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 24(%r11)\n\t"
" movhpd %xmm0, 0(%rbx)\n\t"
" movsd %xmm4, 8(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 56(%r11)\n\t"
" movhpd %xmm1, 32(%rbx)\n\t"
" movsd %xmm5, 40(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 88(%r11)\n\t"
" movhpd %xmm2, 64(%rbx)\n\t"
" movsd %xmm6, 72(%rbx)\n\t"
" je 3f\n\t"
" movsd %xmm3, 120(%r11)\n\t"
" movhpd %xmm3, 96(%rbx)\n\t"
" movsd %xmm7, 104(%rbx)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"6:\n\t"
"\n\t"
" movsd %xmm0, 24(%r11)\n\t"
" movhpd %xmm0, 0(%rbx)\n\t"
" movupd %xmm4, 8(%rbx)\n\t"
" cmpl $ 2, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm1, 56(%r11)\n\t"
" movhpd %xmm1, 32(%rbx)\n\t"
" movupd %xmm5, 40(%rbx)\n\t"
" cmpl $ 3, %r15d\n\t"
" jl 3f\n\t"
" movsd %xmm2, 88(%r11)\n\t"
" movhpd %xmm2, 64(%rbx)\n\t"
" movupd %xmm6, 72(%rbx)\n\t"
" je 3f\n\t"
" movsd %xmm3, 120(%r11)\n\t"
" movhpd %xmm3, 96(%rbx)\n\t"
" movupd %xmm7, 104(%rbx)\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_L_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movapd %xmm4, 16(%r10)\n\t"
" movhpd %xmm1, 40(%r10)\n\t"
" movapd %xmm5, 48(%r10)\n\t"
"\n\t"
" movapd %xmm6, 80(%r10)\n\t"
"\n\t"
" movhpd %xmm7, 120(%r10)\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_L_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 2, %r11d\n\t"
" jg 1f\n\t"
" je 0f\n\t"
"\n\t"
"\n\t"
" movsd %xmm0, 0(%r10)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" cmpl $ 2, %r12d\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" jl 3f\n\t"
" movhpd %xmm1, 40(%r10)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"1:\n\t"
" cmpl $ 3, %r11d\n\t"
" jg 2f\n\t"
"\n\t"
"\n\t"
" cmpl $ 2, %r12d\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movsd %xmm4, 16(%r10)\n\t"
" jl 3f\n\t"
" cmpl $ 3, %r12d\n\t"
" movhpd %xmm1, 40(%r10)\n\t"
" movsd %xmm5, 48(%r10)\n\t"
" jl 3f\n\t"
"\n\t"
" movsd %xmm6, 80(%r10)\n\t"
"\n\t"
" jmp 3f\n\t"
"\n\t"
"2:\n\t"
"\n\t"
" cmpl $ 2, %r12d\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movapd %xmm4, 16(%r10)\n\t"
" jl 3f\n\t"
" cmpl $ 3, %r12d\n\t"
" movhpd %xmm1, 40(%r10)\n\t"
" movapd %xmm5, 48(%r10)\n\t"
" jl 3f\n\t"
"\n\t"
" movapd %xmm6, 80(%r10)\n\t"
" je 3f\n\t"
"\n\t"
" movhpd %xmm7, 120(%r10)\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_TRAN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, %xmm12\n\t"
" unpckhpd %xmm1, %xmm12\n\t"
" unpcklpd %xmm1, %xmm0\n\t"
" movapd %xmm12, %xmm1\n\t"
"\n\t"
" movapd %xmm6, %xmm12\n\t"
" unpckhpd %xmm7, %xmm12\n\t"
" unpcklpd %xmm7, %xmm6\n\t"
" movapd %xmm12, %xmm7\n\t"
"\n\t"
" movapd %xmm2, %xmm12\n\t"
" movapd %xmm2, %xmm13\n\t"
" unpckhpd %xmm3, %xmm12\n\t"
" unpcklpd %xmm3, %xmm13\n\t"
"\n\t"
" movapd %xmm4, %xmm2\n\t"
" unpckhpd %xmm5, %xmm4\n\t"
" unpcklpd %xmm5, %xmm2\n\t"
" movapd %xmm4, %xmm3\n\t"
"\n\t"
" movapd %xmm12, %xmm5\n\t"
" movapd %xmm13, %xmm4\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nt_4x4_lib4; .type kernel_dgemm_nt_4x4_lib4, @function; kernel_dgemm_nt_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nt_4x4_lib4, .-kernel_dgemm_nt_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nt_4x4_vs_lib4; .type kernel_dgemm_nt_4x4_vs_lib4, @function; kernel_dgemm_nt_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
" movq 64 + 24(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nt_4x4_vs_lib4, .-kernel_dgemm_nt_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nt_4x4_gen_lib4; .type kernel_dgemm_nt_4x4_gen_lib4, @function; kernel_dgemm_nt_4x4_gen_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
" movq 64 + 8(%rsp), %r13\n\t"
" movq 64 + 16(%rsp), %r14\n\t"
" sall $ 5, %r14d\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
" movq 64 + 40(%rsp), %r12\n\t"
" sall $ 5, %r12d\n\t"
" movq 64 + 48(%rsp), %r13\n\t"
" movq 64 + 56(%rsp), %r14\n\t"
" movq 64 + 64(%rsp), %r15\n\t"
" movq 64 + 72(%rsp), %rax\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nt_4x4_gen_lib4, .-kernel_dgemm_nt_4x4_gen_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nn_4x4_lib4; .type kernel_dgemm_nn_4x4_lib4, @function; kernel_dgemm_nn_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nn_4x4_lib4, .-kernel_dgemm_nn_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nn_4x4_vs_lib4; .type kernel_dgemm_nn_4x4_vs_lib4, @function; kernel_dgemm_nn_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
" movq 64 + 40(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nn_4x4_vs_lib4, .-kernel_dgemm_nn_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_nn_4x4_gen_lib4; .type kernel_dgemm_nn_4x4_gen_lib4, @function; kernel_dgemm_nn_4x4_gen_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
" movq 64 + 24(%rsp), %r13\n\t"
" movq 64 + 32(%rsp), %r14\n\t"
" sall $ 5, %r14d\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 40(%rsp), %r10\n\t"
" movq 64 + 48(%rsp), %r11\n\t"
" movq 64 + 56(%rsp), %r12\n\t"
" sall $ 5, %r12d\n\t"
" movq 64 + 64(%rsp), %r13\n\t"
" movq 64 + 72(%rsp), %r14\n\t"
" movq 64 + 80(%rsp), %r15\n\t"
" movq 64 + 88(%rsp), %rax\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_nn_4x4_gen_lib4, .-kernel_dgemm_nn_4x4_gen_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_tt_4x4_lib4; .type kernel_dgemm_tt_4x4_lib4, @function; kernel_dgemm_tt_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %r9, %r11\n\t"
" movq %rcx, %r12\n\t"
" movq %r8, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rdx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_TRAN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_tt_4x4_lib4, .-kernel_dgemm_tt_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_tt_4x4_vs_lib4; .type kernel_dgemm_tt_4x4_vs_lib4, @function; kernel_dgemm_tt_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %r9, %r11\n\t"
" movq %rcx, %r12\n\t"
" movq %r8, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rdx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_TRAN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
" movq 64 + 40(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_tt_4x4_vs_lib4, .-kernel_dgemm_tt_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_tt_4x4_gen_lib4; .type kernel_dgemm_tt_4x4_gen_lib4, @function; kernel_dgemm_tt_4x4_gen_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %r9, %r11\n\t"
" movq %rcx, %r12\n\t"
" movq %r8, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rdx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_TRAN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
" movq 64 + 24(%rsp), %r13\n\t"
" movq 64 + 32(%rsp), %r14\n\t"
" sall $ 5, %r14d\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 40(%rsp), %r10\n\t"
" movq 64 + 48(%rsp), %r11\n\t"
" movq 64 + 56(%rsp), %r12\n\t"
" sall $ 5, %r12d\n\t"
" movq 64 + 64(%rsp), %r13\n\t"
" movq 64 + 72(%rsp), %r14\n\t"
" movq 64 + 80(%rsp), %r15\n\t"
" movq 64 + 88(%rsp), %rax\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_tt_4x4_gen_lib4, .-kernel_dgemm_tt_4x4_gen_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dsyrk_nt_l_4x4_lib4; .type kernel_dsyrk_nt_l_4x4_lib4, @function; kernel_dsyrk_nt_l_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dsyrk_nt_l_4x4_lib4, .-kernel_dsyrk_nt_l_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dsyrk_nt_l_4x4_vs_lib4; .type kernel_dsyrk_nt_l_4x4_vs_lib4, @function; kernel_dsyrk_nt_l_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
" movq 64 + 24(%rsp), %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dsyrk_nt_l_4x4_vs_lib4, .-kernel_dsyrk_nt_l_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrmm_nt_ru_4x4_lib4; .type kernel_dtrmm_nt_ru_4x4_lib4, @function; kernel_dtrmm_nt_ru_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" subl $ 4, %r10d\n\t"
" movq %rdx, %r11\n\t"
" addq $ 128, %r11\n\t"
" movq %rcx, %r12\n\t"
" addq $ 128, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdx, %r10\n\t"
" movq %rcx, %r11\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRMM_NT_RU_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r8, %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrmm_nt_ru_4x4_lib4, .-kernel_dtrmm_nt_ru_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrmm_nt_ru_4x4_vs_lib4; .type kernel_dtrmm_nt_ru_4x4_vs_lib4, @function; kernel_dtrmm_nt_ru_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" subl $ 4, %r10d\n\t"
" movq %rdx, %r11\n\t"
" addq $ 128, %r11\n\t"
" movq %rcx, %r12\n\t"
" addq $ 128, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRMM_NT_RU_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r8, %r10\n\t"
" movq %r9, %r11\n\t"
" movq 64 + 8(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrmm_nt_ru_4x4_vs_lib4, .-kernel_dtrmm_nt_ru_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dpotrf_nt_l_4x4_lib4; .type kernel_dpotrf_nt_l_4x4_lib4, @function; kernel_dpotrf_nt_l_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r9, %r10\n\t"
" movl $ 4, %r11d\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DPOTRF_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r8, %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dpotrf_nt_l_4x4_lib4, .-kernel_dpotrf_nt_l_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dpotrf_nt_l_4x4_vs_lib4; .type kernel_dpotrf_nt_l_4x4_vs_lib4, @function; kernel_dpotrf_nt_l_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r9, %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DPOTRF_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r8, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dpotrf_nt_l_4x4_vs_lib4, .-kernel_dpotrf_nt_l_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dsyrk_dpotrf_nt_l_4x4_lib4; .type kernel_dsyrk_dpotrf_nt_l_4x4_lib4, @function; kernel_dsyrk_dpotrf_nt_l_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd .LC11(%rip), %xmm15; xorpd %xmm15, %xmm0; xorpd %xmm15, %xmm1; xorpd %xmm15, %xmm2; xorpd %xmm15, %xmm3; xorpd %xmm15, %xmm4; xorpd %xmm15, %xmm5; xorpd %xmm15, %xmm6; xorpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movl $ 4, %r11d\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DPOTRF_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 16(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dsyrk_dpotrf_nt_l_4x4_lib4, .-kernel_dsyrk_dpotrf_nt_l_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dsyrk_dpotrf_nt_l_4x4_vs_lib4; .type kernel_dsyrk_dpotrf_nt_l_4x4_vs_lib4, @function; kernel_dsyrk_dpotrf_nt_l_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd .LC11(%rip), %xmm15; xorpd %xmm15, %xmm0; xorpd %xmm15, %xmm1; xorpd %xmm15, %xmm2; xorpd %xmm15, %xmm3; xorpd %xmm15, %xmm4; xorpd %xmm15, %xmm5; xorpd %xmm15, %xmm6; xorpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 40(%rsp), %r11\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DPOTRF_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 16(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
" movq 64 + 40(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_L_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dsyrk_dpotrf_nt_l_4x4_vs_lib4, .-kernel_dsyrk_dpotrf_nt_l_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrsm_nt_rl_inv_4x4_lib4; .type kernel_dtrsm_nt_rl_inv_4x4_lib4, @function; kernel_dtrsm_nt_rl_inv_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M1B_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRSM_RLT_INV_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r9, %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrsm_nt_rl_inv_4x4_lib4, .-kernel_dtrsm_nt_rl_inv_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_dtrsm_nt_rl_inv_4x4_lib4; .type kernel_dgemm_dtrsm_nt_rl_inv_4x4_lib4, @function; kernel_dgemm_dtrsm_nt_rl_inv_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd .LC11(%rip), %xmm15; xorpd %xmm15, %xmm0; xorpd %xmm15, %xmm1; xorpd %xmm15, %xmm2; xorpd %xmm15, %xmm3; xorpd %xmm15, %xmm4; xorpd %xmm15, %xmm5; xorpd %xmm15, %xmm6; xorpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRSM_RLT_INV_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 16(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_dtrsm_nt_rl_inv_4x4_lib4, .-kernel_dgemm_dtrsm_nt_rl_inv_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrsm_nt_rl_inv_4x4_vs_lib4; .type kernel_dtrsm_nt_rl_inv_4x4_vs_lib4, @function; kernel_dtrsm_nt_rl_inv_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M1B_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
" movq 64 + 32(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRSM_RLT_INV_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %r9, %r10\n\t"
" movq 64 + 24(%rsp), %r11\n\t"
" movq 64 + 32(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrsm_nt_rl_inv_4x4_vs_lib4, .-kernel_dtrsm_nt_rl_inv_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dgemm_dtrsm_nt_rl_inv_4x4_vs_lib4; .type kernel_dgemm_dtrsm_nt_rl_inv_4x4_vs_lib4, @function; kernel_dgemm_dtrsm_nt_rl_inv_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rsi, %r11\n\t"
" movq %rdx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd .LC11(%rip), %xmm15; xorpd %xmm15, %xmm0; xorpd %xmm15, %xmm1; xorpd %xmm15, %xmm2; xorpd %xmm15, %xmm3; xorpd %xmm15, %xmm4; xorpd %xmm15, %xmm5; xorpd %xmm15, %xmm6; xorpd %xmm15, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_dgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_M11_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
" movq 64 + 32(%rsp), %r11\n\t"
" movq 64 + 48(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRSM_RLT_INV_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 16(%rsp), %r10\n\t"
" movq 64 + 40(%rsp), %r11\n\t"
" movq 64 + 48(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dgemm_dtrsm_nt_rl_inv_4x4_vs_lib4, .-kernel_dgemm_dtrsm_nt_rl_inv_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrmm_nn_rl_4x4_lib4; .type kernel_dtrmm_nn_rl_4x4_lib4, @function; kernel_dtrmm_nn_rl_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRMM_NN_RL_4X4_LIB4\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrmm_nn_rl_4x4_lib4, .-kernel_dtrmm_nn_rl_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrmm_nn_rl_4x4_vs_lib4; .type kernel_dtrmm_nn_rl_4x4_vs_lib4, @function; kernel_dtrmm_nn_rl_4x4_vs_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRMM_NN_RL_4X4_VS_LIB4\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
" movq 64 + 24(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_VS_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrmm_nn_rl_4x4_vs_lib4, .-kernel_dtrmm_nn_rl_4x4_vs_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_dtrmm_nn_rl_4x4_gen_lib4; .type kernel_dtrmm_nn_rl_4x4_gen_lib4, @function; kernel_dtrmm_nn_rl_4x4_gen_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3; movapd %xmm0, %xmm4; movapd %xmm0, %xmm5; movapd %xmm0, %xmm6; movapd %xmm0, %xmm7\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 5, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_DTRMM_NN_RL_4X4_VS_LIB4\n\t"
" call inner_kernel_dgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_A0_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
" movq 64 + 16(%rsp), %r11\n\t"
" movq 64 + 24(%rsp), %r12\n\t"
" sall $ 5, %r12d\n\t"
" movq 64 + 32(%rsp), %r13\n\t"
" movq 64 + 40(%rsp), %r14\n\t"
" movq 64 + 48(%rsp), %r15\n\t"
" movq 64 + 56(%rsp), %rax\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_GEN_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_dtrmm_nn_rl_4x4_gen_lib4, .-kernel_dtrmm_nn_rl_4x4_gen_lib4\n\t"
" .section .rodata.cst32,\"aM\",@progbits,32\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC00:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .quad -1\n\t"
" .quad -1\n\t"
" .quad -1\n\t"
" .quad 1\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC01:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .quad -1\n\t"
" .quad -1\n\t"
" .quad -1\n\t"
" .quad -1\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC02:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .long 0\n\t"
" .long 1071644672\n\t"
" .long 0\n\t"
" .long 1073217536\n\t"
" .long 0\n\t"
" .long 1074003968\n\t"
" .long 0\n\t"
" .long 1074528256\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC03:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .long 0\n\t"
" .long 1074921472\n\t"
" .long 0\n\t"
" .long 1075183616\n\t"
" .long 0\n\t"
" .long 1075445760\n\t"
" .long 0\n\t"
" .long 1075707904\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC04:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .long 0\n\t"
" .long 1072693248\n\t"
" .long 0\n\t"
" .long 1072693248\n\t"
" .long 0\n\t"
" .long 1072693248\n\t"
" .long 0\n\t"
" .long 1072693248\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .align 32\n\t"
".LC11:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" .long 0x0\n\t"
" .long 0x80000000\n\t"
" .long 0x0\n\t"
" .long 0x80000000\n\t"
" .long 0x0\n\t"
" .long 0x80000000\n\t"
" .long 0x0\n\t"
" .long 0x80000000\n\t"
" .long 0x0\n\t"
" .long 0x80000000\n\t"
"\n\t"
"\n\t"
"\n\t"
" .section .note.GNU-stack,\"\",@progbits\n\t");