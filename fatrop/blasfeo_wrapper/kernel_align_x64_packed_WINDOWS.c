__asm__(" .text\n\t"
" .p2align 4,,15\n\t"
" .globl blasfeo_align_2MB\n\t"
" .def blasfeo_align_2MB; .scl 2; .type 32; .endef\n\t"
"blasfeo_align_2MB:\n\t"
"\n\t"
"\n\t"
" subq $256, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp); movq %rdi, 48(%rsp); movq %rsi, 56(%rsp); movups %xmm6, 64(%rsp); movups %xmm7, 80(%rsp); movups %xmm8, 96(%rsp); movups %xmm9, 112(%rsp); movups %xmm10, 128(%rsp); movups %xmm11, 144(%rsp); movups %xmm12, 160(%rsp); movups %xmm13, 176(%rsp); movups %xmm14, 192(%rsp); movups %xmm15, 208(%rsp);\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %rdx, %r11\n\t"
"\n\t"
" addq $ 2097151, %r10\n\t"
" movq $ 2097151, %r12\n\t"
" notq %r12\n\t"
" andq %r12, %r10\n\t"
" movq %r10, (%r11)\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; movq 48(%rsp), %rdi; movq 56(%rsp), %rsi; movups 64(%rsp), %xmm6; movups 80(%rsp), %xmm7; movups 96(%rsp), %xmm8; movups 112(%rsp), %xmm9; movups 128(%rsp), %xmm10; movups 144(%rsp), %xmm11; movups 160(%rsp), %xmm12; movups 176(%rsp), %xmm13; movups 192(%rsp), %xmm14; movups 208(%rsp), %xmm15; addq $256, %rsp;\n\t"
"\n\t"
" ret\n\t"
" .p2align 4,,15\n\t"
" .globl blasfeo_align_4096_byte\n\t"
" .def blasfeo_align_4096_byte; .scl 2; .type 32; .endef\n\t"
"blasfeo_align_4096_byte:\n\t"
"\n\t"
"\n\t"
" subq $256, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp); movq %rdi, 48(%rsp); movq %rsi, 56(%rsp); movups %xmm6, 64(%rsp); movups %xmm7, 80(%rsp); movups %xmm8, 96(%rsp); movups %xmm9, 112(%rsp); movups %xmm10, 128(%rsp); movups %xmm11, 144(%rsp); movups %xmm12, 160(%rsp); movups %xmm13, 176(%rsp); movups %xmm14, 192(%rsp); movups %xmm15, 208(%rsp);\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %rdx, %r11\n\t"
"\n\t"
" addq $ 4095, %r10\n\t"
" movq $ 4095, %r12\n\t"
" notq %r12\n\t"
" andq %r12, %r10\n\t"
" movq %r10, (%r11)\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; movq 48(%rsp), %rdi; movq 56(%rsp), %rsi; movups 64(%rsp), %xmm6; movups 80(%rsp), %xmm7; movups 96(%rsp), %xmm8; movups 112(%rsp), %xmm9; movups 128(%rsp), %xmm10; movups 144(%rsp), %xmm11; movups 160(%rsp), %xmm12; movups 176(%rsp), %xmm13; movups 192(%rsp), %xmm14; movups 208(%rsp), %xmm15; addq $256, %rsp;\n\t"
"\n\t"
" ret\n\t"
" .p2align 4,,15\n\t"
" .globl blasfeo_align_64_byte\n\t"
" .def blasfeo_align_64_byte; .scl 2; .type 32; .endef\n\t"
"blasfeo_align_64_byte:\n\t"
"\n\t"
"\n\t"
" subq $256, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp); movq %rdi, 48(%rsp); movq %rsi, 56(%rsp); movups %xmm6, 64(%rsp); movups %xmm7, 80(%rsp); movups %xmm8, 96(%rsp); movups %xmm9, 112(%rsp); movups %xmm10, 128(%rsp); movups %xmm11, 144(%rsp); movups %xmm12, 160(%rsp); movups %xmm13, 176(%rsp); movups %xmm14, 192(%rsp); movups %xmm15, 208(%rsp);\n\t"
"\n\t"
" movq %rcx, %r10\n\t"
" movq %rdx, %r11\n\t"
"\n\t"
" addq $ 63, %r10\n\t"
" movq $ 63, %r12\n\t"
" notq %r12\n\t"
" andq %r12, %r10\n\t"
" movq %r10, (%r11)\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; movq 48(%rsp), %rdi; movq 56(%rsp), %rsi; movups 64(%rsp), %xmm6; movups 80(%rsp), %xmm7; movups 96(%rsp), %xmm8; movups 112(%rsp), %xmm9; movups 128(%rsp), %xmm10; movups 144(%rsp), %xmm11; movups 160(%rsp), %xmm12; movups 176(%rsp), %xmm13; movups 192(%rsp), %xmm14; movups 208(%rsp), %xmm15; addq $256, %rsp;\n\t"
"\n\t"
" ret\n\t");