__asm__(" .text\n\t"
" .p2align 4,,15\n\t"
" .type inner_kernel_sgemm_nt_4x4_lib4, @function; inner_kernel_sgemm_nt_4x4_lib4:\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
" xorpd %xmm10, %xmm10\n\t"
" movapd %xmm10, %xmm11\n\t"
"\n\t"
"\n\t"
" movaps 0(%r11), %xmm12\n\t"
" movaps 16(%r11), %xmm13\n\t"
" movaps 0(%r12), %xmm8\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
"\n\t"
" .p2align 3\n\t"
"1:\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm12, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm12, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 16(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm12, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm12, %xmm11\n\t"
" movaps 32(%r11), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm13, %xmm8\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm13, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 32(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm13, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm13, %xmm11\n\t"
" movaps 48(%r11), %xmm13\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm12, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm12, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 48(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm12, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm12, %xmm11\n\t"
" movaps 64(%r11), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm13, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm13, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 64(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm13, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm13, %xmm11\n\t"
" movaps 80(%r11), %xmm13\n\t"
"\n\t"
" addq $ 64, %r11\n\t"
" addq $ 64, %r12\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jg 1b\n\t"
"\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" cmpl $ 3, %r10d\n\t"
" jle 4f\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm12, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm12, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 16(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm12, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm12, %xmm11\n\t"
" movaps 32(%r11), %xmm12\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm13, %xmm8\n\t"
" subl $ 4, %r10d\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm13, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 32(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm13, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm13, %xmm11\n\t"
" movaps 48(%r11), %xmm13\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm12, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm12, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" movaps 48(%r12), %xmm8\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm12, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm12, %xmm11\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm13, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm13, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
"\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm13, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm13, %xmm11\n\t"
"\n\t"
"\n\t"
" addq $ 64, %r11\n\t"
" addq $ 64, %r12\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" addps %xmm11, %xmm3\n\t"
"\n\t"
"\n\t"
" jmp 2f\n\t"
"\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 0(%r11), %xmm12\n\t"
" movaps 0(%r12), %xmm8\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" pshufd $ 0x39, %xmm8, %xmm9\n\t"
" mulps %xmm12, %xmm8\n\t"
"\n\t"
" addps %xmm11, %xmm3\n\t"
" pshufd $ 0x39, %xmm9, %xmm10\n\t"
" mulps %xmm12, %xmm9\n\t"
"\n\t"
" addps %xmm8, %xmm0\n\t"
" pshufd $ 0x39, %xmm10, %xmm11\n\t"
" mulps %xmm12, %xmm10\n\t"
"\n\t"
" addps %xmm9, %xmm1\n\t"
" mulps %xmm12, %xmm11\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 16, %r12\n\t"
" addq $ 16, %r11\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jg 3b\n\t"
"\n\t"
"\n\t"
"\n\t"
" addps %xmm10, %xmm2\n\t"
" addps %xmm11, %xmm3\n\t"
"\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size inner_kernel_sgemm_nt_4x4_lib4, .-inner_kernel_sgemm_nt_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .type inner_kernel_sgemm_nn_4x4_lib4, @function; inner_kernel_sgemm_nn_4x4_lib4:\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jle 0f\n\t"
"\n\t"
"\n\t"
" .p2align 3\n\t"
"1:\n\t"
"\n\t"
" prefetcht0 0(%r12, %r13, 2)\n\t"
"\n\t"
"\n\t"
" movaps 0(%r11), %xmm12\n\t"
"\n\t"
" movaps 0(%r12), %xmm8\n\t"
" pshufd $ 0x00, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" movaps 16(%r12), %xmm9\n\t"
" pshufd $ 0x00, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" movaps 32(%r12), %xmm10\n\t"
" pshufd $ 0x00, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" movaps 48(%r12), %xmm11\n\t"
" pshufd $ 0x00, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 16(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0x55, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0x55, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0x55, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0x55, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 32(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 48(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0xff, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0xff, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0xff, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0xff, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
" subl $ 4, %r10d\n\t"
" addq $ 64, %r11\n\t"
" addq %r13, %r12\n\t"
"\n\t"
" cmpl $ 4, %r10d\n\t"
" jg 1b\n\t"
"\n\t"
"\n\t"
"0:\n\t"
"\n\t"
" cmpl $ 3, %r10d\n\t"
" jle 4f\n\t"
"\n\t"
"\n\t"
" movaps 0(%r11), %xmm12\n\t"
"\n\t"
" movaps 0(%r12), %xmm8\n\t"
" pshufd $ 0x00, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" movaps 16(%r12), %xmm9\n\t"
" pshufd $ 0x00, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" movaps 32(%r12), %xmm10\n\t"
" pshufd $ 0x00, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" movaps 48(%r12), %xmm11\n\t"
" pshufd $ 0x00, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 16(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0x55, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0x55, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0x55, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0x55, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 32(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0xaa, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 48(%r11), %xmm12\n\t"
"\n\t"
" pshufd $ 0xff, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" pshufd $ 0xff, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" pshufd $ 0xff, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" pshufd $ 0xff, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
" subl $ 4, %r10d\n\t"
" addq $ 64, %r11\n\t"
" addq %r13, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" jmp 2f\n\t"
"\n\t"
"\n\t"
"4:\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
"\n\t"
"3:\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps 0(%r11), %xmm12\n\t"
"\n\t"
" movss 0(%r12), %xmm8\n\t"
" pshufd $ 0x00, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" movss 16(%r12), %xmm9\n\t"
" pshufd $ 0x00, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" movss 32(%r12), %xmm10\n\t"
" pshufd $ 0x00, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" movss 48(%r12), %xmm11\n\t"
" pshufd $ 0x00, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" addq $ 16, %r11\n\t"
" addq $ 4, %r12\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jg 3b\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size inner_kernel_sgemm_nn_4x4_lib4, .-inner_kernel_sgemm_nn_4x4_lib4\n\t"
" .macro INNER_EDGE_SGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmpl $ 0, %r14d\n\t"
" jle 2f\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
" movl $ 4, %r15d\n\t"
" subl %r14d, %r15d\n\t"
" cmpl %r10d, %r15d\n\t"
"\n\t"
"\n\t"
"\n\t"
" cmovgl %r10d, %r15d\n\t"
"\n\t"
" movl %r14d, %eax\n\t"
" sall $ 2, %eax\n\t"
" addq %rax, %r12\n\t"
"\n\t"
"1:\n\t"
" movaps 0(%r11), %xmm12\n\t"
"\n\t"
" movss 0(%r12), %xmm8\n\t"
" pshufd $ 0x00, %xmm8, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm0\n\t"
"\n\t"
" movss 16(%r12), %xmm9\n\t"
" pshufd $ 0x00, %xmm9, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm1\n\t"
"\n\t"
" movss 32(%r12), %xmm10\n\t"
" pshufd $ 0x00, %xmm10, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm2\n\t"
"\n\t"
" movss 48(%r12), %xmm11\n\t"
" pshufd $ 0x00, %xmm11, %xmm14\n\t"
" mulps %xmm12, %xmm14\n\t"
" addps %xmm14, %xmm3\n\t"
"\n\t"
"\n\t"
" subl $ 1, %r10d\n\t"
" subl $ 1, %r15d\n\t"
" addq $ 16, %r11\n\t"
" addq $ 4, %r12\n\t"
"\n\t"
" cmpl $ 0, %r15d\n\t"
" jg 1b\n\t"
"\n\t"
" cmpl $ 0, %r10d\n\t"
" jle 2f\n\t"
"\n\t"
" addq %r13, %r12\n\t"
" subq $ 16, %r12\n\t"
"\n\t"
"2:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movaps %xmm1, %xmm12\n\t"
" shufps $ 0xd8, %xmm0, %xmm1\n\t"
" shufps $ 0xd8, %xmm3, %xmm0\n\t"
" shufps $ 0xd8, %xmm2, %xmm3\n\t"
" shufps $ 0xd8, %xmm12, %xmm2\n\t"
"\n\t"
" movaps %xmm0, %xmm12\n\t"
" shufps $ 0xd8, %xmm2, %xmm0\n\t"
" shufps $ 0xd8, %xmm12, %xmm2\n\t"
" movaps %xmm1, %xmm12\n\t"
" shufps $ 0xd8, %xmm3, %xmm1\n\t"
" shufps $ 0xd8, %xmm12, %xmm3\n\t"
"\n\t"
"\n\t"
" movss 0(%r10), %xmm15\n\t"
" shufps $ 0x00, %xmm15, %xmm15\n\t"
"\n\t"
" mulps %xmm15, %xmm0\n\t"
" mulps %xmm15, %xmm1\n\t"
" mulps %xmm15, %xmm2\n\t"
" mulps %xmm15, %xmm3\n\t"
"\n\t"
"\n\t"
" movss 0(%r11), %xmm14\n\t"
" shufps $ 0x00, %xmm14, %xmm14\n\t"
"\n\t"
" xorps %xmm15, %xmm15\n\t"
" ucomiss %xmm15, %xmm14\n\t"
" je 0f\n\t"
"\n\t"
" movaps 0(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm0\n\t"
" movaps 16(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm1\n\t"
" movaps 32(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm2\n\t"
" movaps 48(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm3\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movss 0(%r10), %xmm15\n\t"
" shufps $ 0x00, %xmm15, %xmm15\n\t"
"\n\t"
" mulps %xmm15, %xmm0\n\t"
" mulps %xmm15, %xmm1\n\t"
" mulps %xmm15, %xmm2\n\t"
" mulps %xmm15, %xmm3\n\t"
"\n\t"
"\n\t"
" movss 0(%r11), %xmm14\n\t"
" shufps $ 0x00, %xmm14, %xmm14\n\t"
"\n\t"
" xorps %xmm15, %xmm15\n\t"
" ucomiss %xmm15, %xmm14\n\t"
" je 0f\n\t"
"\n\t"
" movaps 0(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm0\n\t"
" movaps 16(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm1\n\t"
" movaps 32(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm2\n\t"
" movaps 48(%r12), %xmm15\n\t"
" mulps %xmm14, %xmm15\n\t"
" addps %xmm15, %xmm3\n\t"
"\n\t"
"0:\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .macro INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movapd %xmm0, 0(%r10)\n\t"
" movapd %xmm1, 16(%r10)\n\t"
" movapd %xmm2, 32(%r10)\n\t"
" movapd %xmm3, 48(%r10)\n\t"
"\n\t"
"\n\t"
" .endm\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_sgemm_nt_4x4_lib4; .type kernel_sgemm_nt_4x4_lib4, @function; kernel_sgemm_nt_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %rcx, %r12\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_sgemm_nt_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq %r8, %r11\n\t"
" movq %r9, %r12\n\t"
"\n\t"
"\n\t"
" INNER_BLEND_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 8(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_sgemm_nt_4x4_lib4, .-kernel_sgemm_nt_4x4_lib4\n\t"
" .p2align 4,,15\n\t"
" .globl kernel_sgemm_nn_4x4_lib4; .type kernel_sgemm_nn_4x4_lib4, @function; kernel_sgemm_nn_4x4_lib4:\n\t"
"\n\t"
" subq $64, %rsp; movq %rbx, (%rsp); movq %rbp, 8(%rsp); movq %r12, 16(%rsp); movq %r13, 24(%rsp); movq %r14, 32(%rsp); movq %r15, 40(%rsp);\n\t"
"\n\t"
"\n\t"
"\n\t"
" xorpd %xmm0, %xmm0; movapd %xmm0, %xmm1; movapd %xmm0, %xmm2; movapd %xmm0, %xmm3\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rdi, %r10\n\t"
" movq %rdx, %r11\n\t"
" movq %r8, %r12\n\t"
" movq %r9, %r13\n\t"
" sall $ 4, %r13d\n\t"
" movq %rcx, %r14\n\t"
"\n\t"
"\n\t"
" INNER_EDGE_SGEMM_NN_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" call inner_kernel_sgemm_nn_4x4_lib4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq %rsi, %r10\n\t"
" movq 64 + 8(%rsp), %r11\n\t"
" movq 64 + 16(%rsp), %r12\n\t"
"\n\t"
"\n\t"
" INNER_SCALE_AB_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq 64 + 24(%rsp), %r10\n\t"
"\n\t"
"\n\t"
" INNER_STORE_4X4_LIB4\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
"\n\t"
" movq (%rsp), %rbx; movq 8(%rsp), %rbp; movq 16(%rsp), %r12; movq 24(%rsp), %r13; movq 32(%rsp), %r14; movq 40(%rsp), %r15; addq $64, %rsp;\n\t"
"\n\t"
" ret\n\t"
"\n\t"
" .size kernel_sgemm_nn_4x4_lib4, .-kernel_sgemm_nn_4x4_lib4\n\t");